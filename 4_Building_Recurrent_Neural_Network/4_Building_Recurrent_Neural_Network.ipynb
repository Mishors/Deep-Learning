{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building Recurrent Neural Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "P5kjMeP5rW0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ]
    },
    {
      "metadata": {
        "id": "B04lWEo1rcNX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BA6_hQvVreyj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Some Utility Functions"
      ]
    },
    {
      "metadata": {
        "id": "3ZCKiAjG-zPW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Tanh Function"
      ]
    },
    {
      "metadata": {
        "id": "So-cbuzp-20p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "  return np.tanh(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egNv7HGGryri",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Softmax Function"
      ]
    },
    {
      "metadata": {
        "id": "nLISwpbqr3zm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hL42Bu-vr90r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Sigmoid Function"
      ]
    },
    {
      "metadata": {
        "id": "LmI4e0yDsAoA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2libwM3_2Ge",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Multiply Two Matricies"
      ]
    },
    {
      "metadata": {
        "id": "42hkwrKL_-jl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def matrix_multiply(x,y):\n",
        "  return np.multiply(x,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VeU4iWWbBS6C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Add Two Matricies"
      ]
    },
    {
      "metadata": {
        "id": "_wnLb4ypBWUo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def matrix_add(x,y):\n",
        "  return np.add(x,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nN5Yyv-rsK4X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Adam Optimizer Utilities"
      ]
    },
    {
      "metadata": {
        "id": "bSHqCwGwsStJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Initialize Adam Optimizer Function"
      ]
    },
    {
      "metadata": {
        "id": "XtL2ulASsai6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two python dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "    \n",
        "    Returns: \n",
        "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    v[\"dWaa\"] = np.zeros(parameters[\"Waa\"].shape)\n",
        "    v[\"dWax\"] = np.zeros(parameters[\"Wax\"].shape)\n",
        "    v[\"dWya\"]= np.zeros(parameters[\"Wya\"].shape)\n",
        "    v[\"dba\"] = np.zeros(parameters[\"ba\"].shape)\n",
        "    v[\"dby\"] = np.zeros(parameters[\"by\"].shape)\n",
        "    \n",
        "    s[\"dWaa\"] = np.zeros(parameters[\"Waa\"].shape)\n",
        "    s[\"dWax\"] = np.zeros(parameters[\"Wax\"].shape)\n",
        "    s[\"dWya\"]= np.zeros(parameters[\"Wya\"].shape)\n",
        "    s[\"dba\"] = np.zeros(parameters[\"ba\"].shape)\n",
        "    s[\"dby\"] = np.zeros(parameters[\"by\"].shape)\n",
        "       \n",
        "    return v, s\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0M7OcJLisnfD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Update Parameters using Adam Opitimizer Function"
      ]
    },
    {
      "metadata": {
        "id": "RavxKgo6q2uQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "    v[\"dWaa\"] = beta1 * v[\"dWaa\"] + (1 - beta1) * grads[\"dWaa\"] \n",
        "    v[\"dWax\"] = beta1 * v[\"dWax\"] + (1 - beta1) * grads[\"dWax\"]\n",
        "    v[\"dWya\"] = beta1 * v[\"dWya\"] + (1 - beta1) * grads[\"dWya\"]\n",
        "    v[\"dba\"] = beta1 * v[\"dba\"] + (1 - beta1) * grads[\"dba\"]\n",
        "    v[\"dby\"] = beta1 * v[\"dby\"] + (1 - beta1) * grads[\"dby\"]\n",
        "        \n",
        "\n",
        "    # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "    v_corrected[\"dWaa\"] = v[\"dWaa\"] / (1 - beta1**t)\n",
        "    v_corrected[\"dWax\"] = v[\"dWax\"] / (1 - beta1**t)\n",
        "    v_corrected[\"dWya\"] = v[\"dWya\"] / (1 - beta1**t)\n",
        "    v_corrected[\"dba\"] = v[\"dba\"] / (1 - beta1**t)\n",
        "    v_corrected[\"dby\"] = v[\"dby\"] / (1 - beta1**t)\n",
        "        \n",
        "\n",
        "    # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "    s[\"dWaa\"] = beta2 * s[\"dWaa\"] + (1 - beta2) * grads[\"dWaa\"] \n",
        "    s[\"dWax\"] = beta2 * s[\"dWax\"] + (1 - beta2) * grads[\"dWax\"]\n",
        "    s[\"dWya\"] = beta2 * s[\"dWya\"] + (1 - beta2) * grads[\"dWya\"]\n",
        "    s[\"dba\"] = beta2 * s[\"dba\"] + (1 - beta2) * grads[\"dba\"]\n",
        "    s[\"dby\"] = beta2 * s[\"dby\"] + (1 - beta2) * grads[\"dby\"]\n",
        "        \n",
        "\n",
        "    # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "    s_corrected[\"dWaa\"] = v[\"dWaa\"] / (1 - beta2**t)\n",
        "    s_corrected[\"dWax\"] = v[\"dWax\"] / (1 - beta2**t)\n",
        "    s_corrected[\"dWya\"] = v[\"dWya\"] / (1 - beta2**t)\n",
        "    s_corrected[\"dba\"] = v[\"dba\"] / (1 - beta2**t)\n",
        "    s_corrected[\"dby\"] = v[\"dby\"] / (1 - beta2**t)\n",
        "        \n",
        "\n",
        "    # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "    parameters[\"Waa\"] = parameters[\"Waa\"] - learning_rate * v_corrected[\"dWaa\"] / np.sqrt(s_corrected[\"dWaa\"] + epsilon)\n",
        "    parameters[\"Wax\"] = parameters[\"Wax\"] - learning_rate * v_corrected[\"dWax\"] / np.sqrt(s_corrected[\"dWax\"] + epsilon)\n",
        "    parameters[\"Wya\"] = parameters[\"Wya\"] - learning_rate * v_corrected[\"dWya\"] / np.sqrt(s_corrected[\"dWya\"] + epsilon)\n",
        "    parameters[\"ba\"]  = parameters[\"ba\"] - learning_rate * v_corrected[\"dba\"] / np.sqrt(s_corrected[\"dba\"] + epsilon)\n",
        "    parameters[\"by\"]  = parameters[\"by\"] - learning_rate * v_corrected[\"dby\"] / np.sqrt(s_corrected[\"dby\"] + epsilon)\n",
        "    \n",
        "        \n",
        "\n",
        "    return parameters, v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G9FkRN8_uZ4u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#RNN Implementation\n"
      ]
    },
    {
      "metadata": {
        "id": "3YB2i-Hlu12H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###RNN Cell \"For Forward Propagation\""
      ]
    },
    {
      "metadata": {
        "id": "sKtr-_L5u5IN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    Returns:\n",
        "    a_next -- next hidden state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    ### START CODE HERE ### \n",
        "    # compute next activation state using the formula given above\n",
        "    linear_combination = np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba\n",
        "    a_next = tanh(linear_combination)\n",
        "    # compute output of the current cell using the formula given above\n",
        "    linear_combination = np.dot(Wya, a_next) + by\n",
        "    yt_pred = softmax(linear_combination)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values you need for backward propagation in cache\n",
        "    cache = (a_next, a_prev, xt, parameters)\n",
        "    \n",
        "    return a_next, yt_pred, cache  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W1YBz7vDvK4N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###RNN Forward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "h5cxQ_UYvP_0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rnn_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "\n",
        "    Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize \"caches\" which will contain the list of all caches\n",
        "    caches = []\n",
        "    \n",
        "    # Retrieve dimensions from shapes of x and Wy\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wya\"].shape\n",
        "    \n",
        "    ### START CODE HERE ###    \n",
        "    # initialize \"a\" and \"y\" with zeros\n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    y_pred = np.zeros((n_y, m, T_x))\n",
        "    \n",
        "    # Initialize a_next \n",
        "    a_next = a0\n",
        "        \n",
        "    # loop over all time-steps\n",
        "    for t in range(T_x):\n",
        "        # Update next hidden state, compute the prediction, get the cache\n",
        "        xt = x[:,:,t]        \n",
        "        a_next, yt_pred, cache = rnn_cell_forward(xt, a_next, parameters)\n",
        "        # Save the value of the new \"next\" hidden state in a\n",
        "        # Save the value of the prediction in y\n",
        "        # Append \"cache\" to \"caches\"\n",
        "        a[:,:,t] = a_next\n",
        "        y_pred[:,:,t] = yt_pred\n",
        "        caches.append(cache)        \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "    \n",
        "    return a, y_pred, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8MqUmcG-vk79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###LSTM Cell \"For Forward Propagation\""
      ]
    },
    {
      "metadata": {
        "id": "U2SVHsOpvndX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
        "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "                        \n",
        "    Returns:\n",
        "    a_next -- next hidden state, of shape (n_a, m)\n",
        "    c_next -- next memory state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
        "    \n",
        "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
        "          c stands for the memory value\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wf = parameters[\"Wf\"]\n",
        "    bf = parameters[\"bf\"]\n",
        "    Wi = parameters[\"Wi\"]\n",
        "    bi = parameters[\"bi\"]\n",
        "    Wc = parameters[\"Wc\"]\n",
        "    bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"]\n",
        "    bo = parameters[\"bo\"]\n",
        "    Wy = parameters[\"Wy\"]\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    # Retrieve dimensions from shapes of xt and Wy\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Concatenate a_prev and xt\n",
        "    a_prev_concat_xt = np.concatenate((a_prev, xt), axis = 0)\n",
        "    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4)\n",
        "    ft = sigmoid(np.dot(Wf, a_prev_concat_xt) + bf)\n",
        "    it = sigmoid(np.dot(Wi, a_prev_concat_xt) + bi)\n",
        "    cct = tanh(np.dot(Wc, a_prev_concat_xt) + bc)\n",
        "    c_next = np.multiply(ft, c_prev) + np.multiply(it, cct)\n",
        "    ot = sigmoid(np.dot(Wo, a_prev_concat_xt) + bo)\n",
        "    a_next = np.multiply(ot, tanh(c_next))\n",
        "        \n",
        "    # Compute prediction of the LSTM cell \n",
        "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eM9meX1Kv18e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###LSTM Forward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "sWviCRkvv9q2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
        "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "                        \n",
        "    Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize \"caches\", which will track the list of all the caches\n",
        "    caches = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from shapes of x and Wy\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wy\"].shape\n",
        " \n",
        "    # initialize \"a\", \"c\" and \"y\" with zeros\n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    c = np.zeros((n_a, m, T_x))\n",
        "    y = np.zeros((n_y, m, T_x))\n",
        " \n",
        "    # Initialize a_next and c_next\n",
        "    a_next = a0\n",
        "    c_next = np.zeros((n_a, m))\n",
        " \n",
        "    # loop over all time-steps\n",
        "    for t in range(T_x):\n",
        "        # Update next hidden state, next memory state, compute the prediction, get the cache\n",
        "        # Save the value of the new \"next\" hidden state in a\n",
        "        # Save the value of the prediction in y \n",
        "        # Save the value of the next cell state \n",
        "        # Append the cache into caches \n",
        "        xt = x[:,:,t]\n",
        "        a_next, c_next, yt_pred, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n",
        "        a[:,:,t] = a_next\n",
        "        c[:,:,t] = c_next\n",
        "        y[:,:,t] = yt_pred\n",
        "        caches.append(cache)       \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return a, y, c, caches  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qKpKxdfywMDT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###RNN Cell \"For Backward Propagation\""
      ]
    },
    {
      "metadata": {
        "id": "pqoy1yQrwYPs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rnn_cell_backward(da_next, cache):\n",
        "    \"\"\"\n",
        "    Implements the backward pass for the RNN-cell (single time-step).\n",
        "\n",
        "    Arguments:\n",
        "    da_next -- Gradient of loss with respect to next hidden state\n",
        "    cache -- python dictionary containing useful values (output of rnn_cell_forward())\n",
        "\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradients of input data, of shape (n_x, m)\n",
        "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
        "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
        "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
        "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve values from cache\n",
        "    (a_next, a_prev, xt, parameters) = cache\n",
        "    \n",
        "    # Retrieve values from parameters\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # compute the gradient of tanh with respect to a_next\n",
        "    dtanh = (1- a_next**2)*da_next\n",
        "    # compute the gradient with respect to x\n",
        "    dxt = np.dot(np.transpose(Wax),dtanh)\n",
        "    # compute the gradient of the loss with respect to Wax \n",
        "    dWax = np.dot(dtanh,np.transpose(xt))    \n",
        "    # compute the gradient with respect to a_prev\n",
        "    da_prev = np.dot(np.transpose(Waa),dtanh)\n",
        "    # compute the gradient with respect to Waa \n",
        "    dWaa = np.dot(dtanh,np.transpose(a_prev))\n",
        "    # compute the gradient with respect to b\n",
        "    dba = np.sum(dtanh, axis = 1,keepdims=1)\n",
        "   \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
        "    \n",
        "    return gradients\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXNy3M6SwdEH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###RNN Backward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "CWGnWGSjwkiV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rnn_backward(da, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
        "\n",
        "    Arguments:\n",
        "    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n",
        "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
        "    \n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)\n",
        "                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)\n",
        "                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)\n",
        "                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)\n",
        "                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "        \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Retrieve values from the first cache (t=1) of caches\n",
        "    (caches, x) = caches\n",
        "    (a1, a0, x1, parameters) = caches[0]\n",
        "    \n",
        "    # Retrieve dimensions from da's and x1's shapes\n",
        "    n_a, m, T_x = da.shape\n",
        "    n_x, m = x1.shape\n",
        "   \n",
        "    # initialize the gradients with the right sizes \n",
        "    dx = np.zeros((n_x, m, T_x))\n",
        "    dWax = np.zeros((n_a, n_x))\n",
        "    dWaa = np.zeros((n_a, n_a))\n",
        "    dba = np.zeros((n_a, 1))\n",
        "    da0 = np.zeros((n_a, m))\n",
        "    da_prev = np.zeros((n_a, m))\n",
        "    \n",
        "    # Loop through all the time steps\n",
        "    for t in reversed(range(T_x)):\n",
        "        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step.\n",
        "        gradients = rnn_cell_backward(da[:,:,t] + da_prev, caches[t])\n",
        "        # Retrieve derivatives from gradients\n",
        "        dxt = gradients[\"dxt\"]\n",
        "        da_prev = gradients[\"da_prev\"]\n",
        "        dWaxt = gradients[\"dWax\"]\n",
        "        dWaat = gradients[\"dWaa\"]\n",
        "        dbat =  gradients[\"dba\"]\n",
        "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t\n",
        "        dx[:, :, t] = dxt\n",
        "        dWax += dWaxt\n",
        "        dWaa += dWaat\n",
        "        dba += dbat\n",
        "        \n",
        "    # Set da0 to the gradient of a which has been backpropagated through all time-steps \n",
        "    da0 = da_prev\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oNS2CV8Ad2ST",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###GRU Cell \"For Forward Propagation\""
      ]
    },
    {
      "metadata": {
        "id": "IXFhCI5rd-ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gru_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wr -- Weight matrix of the reset gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        br -- Bias of the reset gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)                        \n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "                        \n",
        "    Returns:\n",
        "    a_next -- next hidden state, of shape (n_a, m)\n",
        "    c_next -- next memory state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
        "    \n",
        "    Note: it/rt stand for the update/reset gates, cct stands for the candidate value (c tilde),\n",
        "          c stands for the memory value\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wi = parameters[\"Wi\"]\n",
        "    bi = parameters[\"bi\"]\n",
        "    Wr = parameters[\"Wr\"]\n",
        "    br = parameters[\"br\"]\n",
        "    Wc = parameters[\"Wc\"]\n",
        "    bc = parameters[\"bc\"]    \n",
        "    Wy = parameters[\"Wy\"]\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    # Retrieve dimensions from shapes of xt and Wy\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Concatenate a_prev and xt\n",
        "    a_prev_concat_xt = np.concatenate((a_prev, xt), axis = 0)\n",
        "    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4)\n",
        "    it = sigmoid(np.dot(Wi, a_prev_concat_xt) + bi)\n",
        "    rt = sigmoid(np.dot(Wr, a_prev_concat_xt) + br)\n",
        "    a_prev_concat_xt = np.concatenate((rt*a_prev, xt), axis = 0)\n",
        "    cct = tanh(np.dot(Wc, a_prev_concat_xt) + bc)\n",
        "    c_next = np.multiply(1-it, c_prev) + np.multiply(it, cct)    \n",
        "    a_next = c_next\n",
        "        \n",
        "    # Compute prediction of the LSTM cell \n",
        "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    cache = (a_next, c_next, a_prev, c_prev, it, rt, cct, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RQHPceKkpJ0F",
        "colab_type": "code",
        "outputId": "a3ac506b-9fce-4d0c-93d3-4c359c062720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "c_prev = np.random.randn(5,10)\n",
        "Wi = np.random.randn(5, 5+3)\n",
        "bi = np.random.randn(5,1)\n",
        "Wr = np.random.randn(5, 5+3)\n",
        "br = np.random.randn(5,1)\n",
        "Wc = np.random.randn(5, 5+3)\n",
        "bc = np.random.randn(5,1)\n",
        "Wy = np.random.randn(2,5)\n",
        "by = np.random.randn(2,1)\n",
        "\n",
        "parameters = {\"Wi\": Wi, \"Wr\": Wr, \"Wc\": Wc, \"Wy\": Wy, \"bi\": bi, \"br\": br, \"bc\": bc, \"by\": by}\n",
        "\n",
        "a_next, c_next, yt, cache = gru_cell_forward(xt, a_prev, c_prev, parameters)\n",
        "print(\"a_next[4] = \", a_next[4])\n",
        "print(\"a_next.shape = \", c_next.shape)\n",
        "print(\"c_next[2] = \", c_next[2])\n",
        "print(\"c_next.shape = \", c_next.shape)\n",
        "print(\"yt[1] =\", yt[1])\n",
        "print(\"yt.shape = \", yt.shape)\n",
        "print(\"cache[1][3] =\", cache[1][3])\n",
        "print(\"len(cache) = \", len(cache))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_next[4] =  [-0.15888041 -0.77528599  1.17801636  1.84608963  0.18076577  1.13656495\n",
            "  1.32458182  0.30206573 -1.05058664  0.84694626]\n",
            "a_next.shape =  (5, 10)\n",
            "c_next[2] =  [-0.33743391  0.90442564  0.79432357  0.73607839  0.01229378  0.99282393\n",
            "  0.51857879  0.75727845  0.51477458 -0.98020839]\n",
            "c_next.shape =  (5, 10)\n",
            "yt[1] = [0.435091   0.49819562 0.34331434 0.19653055 0.22972231 0.42514107\n",
            " 0.57137278 0.31652118 0.20597662 0.08644453]\n",
            "yt.shape =  (2, 10)\n",
            "cache[1][3] = [-1.32832624 -0.06765877  0.59319392  0.15677746  0.29191955 -0.28954062\n",
            " -0.09447678  0.51285398  0.03396127  0.47950497]\n",
            "len(cache) =  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rgn_SXg-eD2R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###GRU Forward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "EkEb2XOieF1E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gru_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wr -- Weight matrix of the reset gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        br -- Bias of the reset gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)                        \n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "                        \n",
        "    Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize \"caches\", which will track the list of all the caches\n",
        "    caches = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from shapes of x and Wy\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wy\"].shape\n",
        " \n",
        "    # initialize \"a\", \"c\" and \"y\" with zeros\n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    c = np.zeros((n_a, m, T_x))\n",
        "    y = np.zeros((n_y, m, T_x))\n",
        " \n",
        "    # Initialize a_next and c_next\n",
        "    a_next = a0\n",
        "    c_next = np.zeros((n_a, m))\n",
        " \n",
        "    # loop over all time-steps\n",
        "    for t in range(T_x):\n",
        "        # Update next hidden state, next memory state, compute the prediction, get the cache\n",
        "        # Save the value of the new \"next\" hidden state in a\n",
        "        # Save the value of the prediction in y \n",
        "        # Save the value of the next cell state \n",
        "        # Append the cache into caches \n",
        "        xt = x[:,:,t]\n",
        "        a_next, c_next, yt_pred, cache = gru_cell_forward(xt, a_next, c_next, parameters)\n",
        "        a[:,:,t] = a_next\n",
        "        c[:,:,t] = c_next\n",
        "        y[:,:,t] = yt_pred\n",
        "        caches.append(cache)       \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return a, y, c, caches  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lWP9t63vp2wB",
        "colab_type": "code",
        "outputId": "af26bfa2-4497-48f8-b64c-6c3edf67f06e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,7)\n",
        "a0 = np.random.randn(5,10)\n",
        "Wf = np.random.randn(5, 5+3)\n",
        "bf = np.random.randn(5,1)\n",
        "Wi = np.random.randn(5, 5+3)\n",
        "bi = np.random.randn(5,1)\n",
        "Wr = np.random.randn(5, 5+3)\n",
        "br = np.random.randn(5,1)\n",
        "Wc = np.random.randn(5, 5+3)\n",
        "bc = np.random.randn(5,1)\n",
        "Wy = np.random.randn(2,5)\n",
        "by = np.random.randn(2,1)\n",
        "\n",
        "parameters = {\"Wi\": Wi, \"Wr\": Wr, \"Wc\": Wc, \"Wy\": Wy, \"bi\": bi, \"br\": br, \"bc\": bc, \"by\": by}\n",
        "\n",
        "a, y, c, caches = gru_forward(x, a0, parameters)\n",
        "print(\"a[4][3][6] = \", a[4][3][6])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y[1][4][3] =\", y[1][4][3])\n",
        "print(\"y.shape = \", y.shape)\n",
        "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
        "print(\"c[1][2][1]\", c[1][2][1])\n",
        "print(\"len(caches) = \", len(caches))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][3][6] =  0.9277609910379548\n",
            "a.shape =  (5, 10, 7)\n",
            "y[1][4][3] = 0.9885310594368569\n",
            "y.shape =  (2, 10, 7)\n",
            "caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
            "  0.41005165]\n",
            "c[1][2][1] -0.6264956687103556\n",
            "len(caches) =  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PDzPz8CCwvoi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ]
    },
    {
      "metadata": {
        "id": "Pa8JqKIdujiO",
        "colab_type": "code",
        "outputId": "771dead0-492f-4d32-8d4e-ba4df58f6fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wax = np.random.randn(5,3)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
        "print(\"a_next[4] = \", a_next[4])\n",
        "print(\"a_next.shape = \", a_next.shape)\n",
        "print(\"yt_pred[1] =\", yt_pred[1])\n",
        "print(\"yt_pred.shape = \", yt_pred.shape)\n",
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,4)\n",
        "a0 = np.random.randn(5,10)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wax = np.random.randn(5,3)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
        "print(\"a[4][1] = \", a[4][1])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
        "print(\"y_pred.shape = \", y_pred.shape)\n",
        "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
        "print(\"len(caches) = \", len(caches))\n",
        "\n",
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "c_prev = np.random.randn(5,10)\n",
        "Wf = np.random.randn(5, 5+3)\n",
        "bf = np.random.randn(5,1)\n",
        "Wi = np.random.randn(5, 5+3)\n",
        "bi = np.random.randn(5,1)\n",
        "Wo = np.random.randn(5, 5+3)\n",
        "bo = np.random.randn(5,1)\n",
        "Wc = np.random.randn(5, 5+3)\n",
        "bc = np.random.randn(5,1)\n",
        "Wy = np.random.randn(2,5)\n",
        "by = np.random.randn(2,1)\n",
        "\n",
        "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
        "\n",
        "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
        "print(\"a_next[4] = \", a_next[4])\n",
        "print(\"a_next.shape = \", c_next.shape)\n",
        "print(\"c_next[2] = \", c_next[2])\n",
        "print(\"c_next.shape = \", c_next.shape)\n",
        "print(\"yt[1] =\", yt[1])\n",
        "print(\"yt.shape = \", yt.shape)\n",
        "print(\"cache[1][3] =\", cache[1][3])\n",
        "print(\"len(cache) = \", len(cache))\n",
        "\n",
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,7)\n",
        "a0 = np.random.randn(5,10)\n",
        "Wf = np.random.randn(5, 5+3)\n",
        "bf = np.random.randn(5,1)\n",
        "Wi = np.random.randn(5, 5+3)\n",
        "bi = np.random.randn(5,1)\n",
        "Wo = np.random.randn(5, 5+3)\n",
        "bo = np.random.randn(5,1)\n",
        "Wc = np.random.randn(5, 5+3)\n",
        "bc = np.random.randn(5,1)\n",
        "Wy = np.random.randn(2,5)\n",
        "by = np.random.randn(2,1)\n",
        "\n",
        "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
        "\n",
        "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
        "print(\"a[4][3][6] = \", a[4][3][6])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y[1][4][3] =\", y[1][4][3])\n",
        "print(\"y.shape = \", y.shape)\n",
        "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
        "print(\"c[1][2][1]\", c[1][2][1])\n",
        "print(\"len(caches) = \", len(caches))\n",
        "\n",
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "Wax = np.random.randn(5,3)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wya = np.random.randn(2,5)\n",
        "b = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
        "\n",
        "da_next = np.random.randn(5,10)\n",
        "gradients = rnn_cell_backward(da_next, cache)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)\n",
        "\n",
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,4)\n",
        "a0 = np.random.randn(5,10)\n",
        "Wax = np.random.randn(5,3)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "a, y, caches = rnn_forward(x, a0, parameters)\n",
        "da = np.random.randn(5, 10, 4)\n",
        "gradients = rnn_backward(da, caches)\n",
        "\n",
        "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
        "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
        "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
        "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
            " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
            "a_next.shape =  (5, 10)\n",
            "yt_pred[1] = [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
            " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
            "yt_pred.shape =  (2, 10)\n",
            "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
            "a.shape =  (5, 10, 4)\n",
            "y_pred[1][3] = [0.79560373 0.86224861 0.11118257 0.81515947]\n",
            "y_pred.shape =  (2, 10, 4)\n",
            "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
            "len(caches) =  2\n",
            "a_next[4] =  [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
            "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
            "a_next.shape =  (5, 10)\n",
            "c_next[2] =  [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
            "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
            "c_next.shape =  (5, 10)\n",
            "yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n",
            " 0.00943007 0.12666353 0.39380172 0.07828381]\n",
            "yt.shape =  (2, 10)\n",
            "cache[1][3] = [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
            "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
            "len(cache) =  10\n",
            "a[4][3][6] =  0.17211776753291672\n",
            "a.shape =  (5, 10, 7)\n",
            "y[1][4][3] = 0.9508734618501101\n",
            "y.shape =  (2, 10, 7)\n",
            "caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
            "  0.41005165]\n",
            "c[1][2][1] -0.8555449167181981\n",
            "len(caches) =  2\n",
            "gradients[\"dxt\"][1][2] = -0.4605641030588796\n",
            "gradients[\"dxt\"].shape = (3, 10)\n",
            "gradients[\"da_prev\"][2][3] = 0.08429686538067724\n",
            "gradients[\"da_prev\"].shape = (5, 10)\n",
            "gradients[\"dWax\"][3][1] = 0.39308187392193034\n",
            "gradients[\"dWax\"].shape = (5, 3)\n",
            "gradients[\"dWaa\"][1][2] = -0.28483955786960663\n",
            "gradients[\"dWaa\"].shape = (5, 5)\n",
            "gradients[\"dba\"][4] = [0.80517166]\n",
            "gradients[\"dba\"].shape = (5, 1)\n",
            "gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
            "gradients[\"dx\"].shape = (3, 10, 4)\n",
            "gradients[\"da0\"][2][3] = -0.31494237512664996\n",
            "gradients[\"da0\"].shape = (5, 10)\n",
            "gradients[\"dWax\"][3][1] = 11.264104496527777\n",
            "gradients[\"dWax\"].shape = (5, 3)\n",
            "gradients[\"dWaa\"][1][2] = 2.303333126579893\n",
            "gradients[\"dWaa\"].shape = (5, 5)\n",
            "gradients[\"dba\"][4] = [-0.74747722]\n",
            "gradients[\"dba\"].shape = (5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lPIDRTIFvCqj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#IMDB Dataset"
      ]
    },
    {
      "metadata": {
        "id": "JIal6OROvGcL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TAj-NgDzzTO8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9d83cc39-d1a6-4b95-b2fb-f4aa2cbe58aa"
      },
      "cell_type": "code",
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kxz-yEWov7mn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ehkzRPjEzc_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "46d379fa-0c40-470e-a5d1-f1381793881b"
      },
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 414s 17ms/step - loss: 0.4680 - acc: 0.7774\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 415s 17ms/step - loss: 0.3370 - acc: 0.8593\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 413s 17ms/step - loss: 0.2760 - acc: 0.8867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7effe5dba080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "gdp_nUXJzgnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d085e664-ec5c-4c07-d5e4-4084fb9e4399"
      },
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 86.24%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}